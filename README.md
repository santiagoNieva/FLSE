# FLSE ‚Äî Fractal Layered Spherical Embeddings

FLSE es un modelo experimental de *embeddings fractales jer√°rquicos* basado en:

- capas sem√°nticas m√∫ltiples,
- geometr√≠a esf√©rica en alta dimensi√≥n,
- representaci√≥n fractal del significado,
- distilaci√≥n desde embeddings teacher (GloVe, fastText, LaBSE, etc.),
- control expl√≠cito de polisemia y granularidad,
- entrop√≠a por capa como regulador estructural.

El objetivo del proyecto es explorar una alternativa te√≥rica y pr√°ctica a los
embeddings tradicionales (GloVe / word2vec / fastText / BERT embeddings),
desacoplando:

1. significado macro (conceptos generales),
2. significado meso (categor√≠as y subcategor√≠as estables),
3. significado micro (polisemia contextual, jergas, dominio espec√≠fico),

mediante un *espacio fractal* compuesto por m√∫ltiples hiperesferas conectadas.

---

## ‚ú® Motivaci√≥n

Los modelos modernos de lenguaje usan grandes espacios vectoriales altamente
entrelazados donde:

- nociones generales,
- conceptos espec√≠ficos,
- jergas contextuales,
- relaciones sint√°cticas

se mezclan en un √∫nico embedding dif√≠cil de interpretar.

FLSE propone una descomposici√≥n estructurada:

- cada palabra se representa mediante un vector compuesto (una capa = una esfera),
- cada capa escoge una combinaci√≥n suave de v√©rtices,
- la combinaci√≥n se regula con una entrop√≠a objetivo (capa alta = distribuciones amplias, capa baja = distribuciones concentradas),
- el embedding final es la concatenaci√≥n de todas las capas.

Esto permite:

- interpretar significado seg√∫n escala,
- manejar polisemia expl√≠cita,
- robustez a spanglish, jergas y mezclas culturales,
- especializaci√≥n progresiva sin interferir con capas superiores,
- integraci√≥n sencilla con embeddings teacher multiling√ºes.

---

## üìê Arquitectura

```
Capa 1 (macro)       ‚Üí v√©rtices esf√©ricos, sem√°ntica general
input ‚Üí Capa 2 (meso)‚Üí categor√≠as y subcategor√≠as
Capa 3 (micro)       ‚Üí jergas, uso espec√≠fico, dominios
...
Capa N (fina)        ‚Üí detalles contextuales, sintaxis opcional

Embeddings finales = concat(capa1, capa2, ... capaN)
```

Cada capa tiene:

- `V` v√©rtices distribuidos sobre una hiperesfera `D`‚Äìdimensional
- para cada palabra se aprenden `V` logits
- se aplica `softmax` ‚Üí pesos por v√©rtice
- el embedding de capa es la mezcla convexa de sus v√©rtices

---

## üß† Entrenamiento

FLSE se entrena mediante **distillation** desde un embedding teacher:

- GloVe (ingl√©s)
- fastText (multiling√ºe)
- LaBSE (multiling√ºe alineado)
- SBERT multilingual

La p√©rdida incluye:

1. MSE entre FLSE y el embedding teacher
2. Regularizaci√≥n de entrop√≠a por capa

```
loss = MSE(FLSE, teacher) + Œª * (Entropy_per_layer - TargetEntropy)¬≤
```

Esto fuerza a cada capa a aprender un nivel controlado de granularidad, estabilizando el espacio fractal.

---

## üöÄ Getting Started

### Instalaci√≥n

Clonar el repositorio:

```bash
git clone https://github.com/santiagoNieva/FLSE.git
cd FLSE
```

### Usando Poetry

```bash
poetry install
```

Activar entorno:

```bash
source $(poetry env info --path)/bin/activate
```

### Ejecutar tests

```bash
make test
```

---

## üß† Uso b√°sico

```python
import torch
from flse.geometry import generate_vertices
from flse.model import FLSEModel

vertices = generate_vertices(num_layers=3, verts_per_layer=16, dim=16)
model = FLSEModel(vocab_size=1000, vertices=vertices, teacher_dim=64)

embedding = model.flse_embedding(torch.tensor(42))
print(embedding.shape)
```

---

## ‚òÅÔ∏è FLSE en Google Colab

```python
!git clone https://github.com/santiagoNieva/FLSE.git
%cd FLSE
!pip install -e .
```

Importar:

```python
from flse.model import FLSEModel
from flse.geometry import generate_vertices
```

---

## üß™ Playground r√°pido con un teacher

Hay un script sencillo para jugar con par√°metros y un embedding teacher (propio o
aleatorio):

```bash
# Teacher aleatorio para smoke-test r√°pido
python experiments/distill_playground.py --vocab-size 500 --num-layers 3 --verts-per-layer 16 --dim 16 --teacher-dim 64 --epochs 3 --target-entropy 1.2

# Usando un teacher guardado en .npy (shape: vocab, dim)
python experiments/distill_playground.py --teacher-path data/teacher.npy --num-layers 4 --verts-per-layer 24 --dim 16 --epochs 5 --lambda-ent 0.2 --target-entropies 1.5 1.0 0.8 0.5
```

Tips:

- `--device auto|cpu|cuda` elige la aceleraci√≥n (auto usa CUDA si est√° disponible).
- Las entrop√≠as objetivo pueden ser una sola (`--target-entropy`) o una lista
  por capa (`--target-entropies ...`). Sirven para controlar la mezcla suave de
  v√©rtices en cada nivel.
- El teacher se carga desde un `.npy` de shape `(vocab, dim)`. Pod√©s recortar con
  `--vocab-size` si quer√©s probar solo un subconjunto.

---

## üìú Licencia

Este proyecto se publica bajo **Creative Commons Attribution‚ÄìNonCommercial 4.0 (CC BY-NC 4.0)**.

Esto implica:

- pod√©s usar el c√≥digo para fines personales, acad√©micos o experimentales,
- pod√©s modificar y redistribuir derivaciones bajo la misma licencia,
- NO est√° permitido el uso comercial sin autorizaci√≥n expl√≠cita del autor,
- empresas o instituciones deben solicitar una licencia comercial.

La licencia podr√° revisarse cuando el proyecto alcance mayor madurez.

---
