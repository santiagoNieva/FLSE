# FLSE ‚Äî Fractal Layered Spherical Embeddings

FLSE es un modelo experimental de *embeddings fractales jer√°rquicos* basado en:

- **capas sem√°nticas m√∫ltiples**,  
- **geometr√≠a esf√©rica en alta dimensi√≥n**,  
- **representaci√≥n fractal del significado**,  
- **distilaci√≥n desde embeddings teacher** (GloVe, fastText, LaBSE, etc.),  
- **control expl√≠cito de polisemia y granularidad**,  
- **entrop√≠a por capa como regulador estructural**.

El objetivo del proyecto es explorar una alternativa te√≥rica y pr√°ctica a los
embeddings tradicionales (GloVe / word2vec / fastText / BERT embeddings),
desacoplando:

1. **significado macro** (conceptos generales),  
2. **significado meso** (categor√≠as y subcategor√≠as estables),  
3. **significado micro** (polisemia contextual, jergas, dominio espec√≠fico),  

mediante un **espacio fractal** compuesto por m√∫ltiples hiperesferas conectadas.

---

## ‚ú® Motivaci√≥n

Los modelos modernos de lenguaje usan grandes espacios vectoriales altamente
entrelazados donde:

- las nociones generales,  
- las espec√≠ficas,  
- las jergas contextuales,  
- y las relaciones sint√°cticas  

se mezclan en un √∫nico embedding dif√≠cil de interpretar.

FLSE propone una descomposici√≥n estructurada:

- cada palabra se representa mediante un **vector compuesto** (una capa = una esfera),
- cada capa escoge una **combination suave de v√©rtices**,  
- la combinaci√≥n se regula con una **entrop√≠a objetivo** (capa alta = distribuciones amplias, capa baja = distribuciones concentradas),
- el embedding final es la **concatenaci√≥n de todas las capas**.

Esto permite:

- interpretaci√≥n por escala sem√°ntica,
- polisemia expl√≠cita,
- robustez a *spanglish*, jergas y mezclas culturales,
- especializaci√≥n progresiva sin interferir con capas superiores,
- integraci√≥n sencilla con modelos teacher multiling√ºes.

---

## üìê Arquitectura

```
Capa 1 (macro)       ‚Üí v√©rtices esf√©ricos, sem√°ntica general
input ‚Üí Capa 2 (meso)‚Üí categor√≠as y subcategor√≠as
Capa 3 (micro)       ‚Üí jergas, uso espec√≠fico, dominios
...
Capa N (fina)        ‚Üí detalles contextuales, sintaxis opcional

Embeddings finales = concat(capa1, capa2, ... capaN)
```

Cada capa tiene:

- `V` v√©rtices distribuidos sobre una hiperesfera `D`‚Äìdimensional  
- para cada palabra se aprenden `V` logits  
- se aplica `softmax` ‚Üí pesos por v√©rtice  
- el embedding de capa es la mezcla convexa de sus v√©rtices  

---

## üß† Entrenamiento

FLSE se entrena por **distillation** desde un embedding teacher:

- GloVe (ingl√©s)
- fastText (multiling√ºe, espa√±ol incluido)
- LaBSE (multiling√ºe alineado)
- SBERT multilingual, etc.

La p√©rdida incluye:

1. **MSE** entre FLSE y el teacher  
2. **Regularizaci√≥n de entrop√≠a por capa**

```
loss = MSE(FLSE, teacher) + Œª * (Entropy_per_layer - TargetEntropy)¬≤
```

---

## üìú Licencia

Este proyecto se publica bajo **Creative Commons Attribution‚ÄìNonCommercial 4.0 (CC BY-NC 4.0)**.

Esto significa:

- pod√©s leer, estudiar y modificar el c√≥digo,
- pod√©s usarlo con fines acad√©micos, personales o experimentales,
- **NO** est√° permitido el uso comercial sin autorizaci√≥n expresa del autor,
- las empresas o instituciones que deseen integrarlo en productos deber√°n solicitar una licencia comercial.

Este esquema es temporal durante la etapa de investigaci√≥n del proyecto.
