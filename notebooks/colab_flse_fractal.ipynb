{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLSE distillation en Colab (fastText es)\n",
    "\n",
    "Notas y bloques básicos que usamos para jugar con 5 capas, temperaturas por capa y vecinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup repositorio y deps\n",
    "Clonar el repo y preparar entorno editable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/santiagoNieva/FLSE.git\n",
    "%cd FLSE\n",
    "!pip install -e .\n",
    "\n",
    "# (opcional) limpiar kernel si ya había versiones previas cargadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Descargar teacher fastText (es) y generar .npy\n",
    "Guardamos top 200k palabras para acelerar corridas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O /content/cc.es.300.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz\n",
    "!gunzip /content/cc.es.300.bin.gz\n",
    "\n", 
    "import fasttext, numpy as np\n",
    "ft = fasttext.load_model('/content/cc.es.300.bin')\n",
    "vocab = ft.words[:200000]\n",
    "vecs = np.vstack([ft.get_word_vector(w) for w in vocab])\n",
    "np.save('/content/FLSE/teacher_fasttext_es_200k.npy', vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Entrenar FLSE (5 capas, temps decrecientes para afilar)\n",
    "Bloque \"soft\" que dejó entropías ~[1.59, 0.99, 0.94, 0.64, 0.14] en 200k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experiments/distill_playground.py \\\n",
    "  --teacher-path /content/FLSE/teacher_fasttext_es_200k.npy \\\n",
    "  --vocab-size 200000 --teacher-dim 300 \\\n",
    "  --num-layers 5 --verts-per-layer 12 --dim 12 \\\n",
    "  --epochs 12 --batch-size 64 \\\n",
    "  --lambda-ent 9.0 \\\n",
    "  --lambda-entropies 1.0 1.5 2.5 3.0 3.0 \\\n",
    "  --target-entropies 1.6 1.0 0.8 0.5 0.40 \\\n",
    "  --logit-temps 1.0 1.5 2.0 2.5 2.7 \\\n",
    "  --device cuda \\\n",
    "  --save-path /content/FLSE/data/flse_fasttext_5c_200k_soft.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Cargar checkpoint y ver entropías/top-k vértices\n",
    "Incluye filtrado de vocab para vecinos alfabéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, numpy as np, fasttext\n",
    "from flse.model import FLSEModel\n",
    "from flse.geometry import generate_vertices\n",
    "from experiments.inspect_word_geometry import inspect_word\n",
    "\n",
    "ckpt = torch.load('/content/FLSE/data/flse_fasttext_5c_200k_soft.pt', map_location='cpu')\n",
    "cfg = ckpt['config']\n",
    "temps = None if cfg.get('logit_temps') is None else torch.tensor(cfg['logit_temps'])\n",
    "vertices = generate_vertices(cfg['num_layers'], cfg['verts_per_layer'], cfg['dim'])\n",
    "model = FLSEModel(cfg['vocab_size'], vertices, cfg['teacher_dim'], logit_temps=temps)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "ft = fasttext.load_model('/content/cc.es.300.bin')\n",
    "words = ft.words[:cfg['vocab_size']]\n",
    "keep = [i for i,w in enumerate(words) if w.isalpha() and len(w)>2]\n",
    "idx_keep = torch.tensor(keep)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reps = F.normalize(model(idx_keep), dim=1)\n",
    "    teacher_np = np.stack([ft.get_word_vector(words[i]) for i in keep])\n",
    "    teacher = F.normalize(torch.from_numpy(teacher_np), dim=1)\n",
    "\n",
    "def top_flse(word, k=10):\n",
    "    idx = ft.get_word_id(word)\n",
    "    if idx < 0 or idx >= cfg['vocab_size'] or idx not in keep:\n",
    "        return []\n",
    "    pos = keep.index(idx)\n",
    "    sims = torch.mv(reps, reps[pos])\n",
    "    top = sims.topk(k).indices.tolist()\n",
    "    return [words[keep[i]] for i in top]\n",
    "\n",
    "for w in ['perro', 'gato', 'animal', 'policía', 'gorra']:\n",
    "    print(w, '→', top_flse(w))\n",
    "    idx = ft.get_word_id(w)\n",
    "    if idx >= 0:\n",
    "        inspect_word(model, word_idx=idx, topk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Ajustes rápidos\n",
    "- Si la capa micro colapsa: baja última temp (ej. 2.3) o sube target final (0.35–0.4).\n",
    "- Si queda plana: sube temps profundas o pesos `lambda-entropies`.\n",
    "- Para vocab limpio, genera un teacher filtrado (solo tokens alfabéticos) y distila sobre ese subset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
